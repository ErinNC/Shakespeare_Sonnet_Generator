{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "nteract": {
      "version": "0.23.3"
    },
    "colab": {
      "name": "Shakespeare_Sonnet_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nbsDADwcqLJ"
      },
      "source": [
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "\n",
        "Our goal in this project is to build a Shakespeare Sonnet Generator using **Recurrent Neural Networks and LSTMs**.<br>\n",
        "Given a prompt of a few words as input, its task is to generate follow-on text that reads like a Shakespeare Sonnet!<br>\n",
        "\n",
        "\n",
        "To build our Sonnet Generator we will use a **sequence model**. Given a short sequence, a sequence  model predicts the **most likely next item in the sequence**. Sequence models are astonishingly versatile and powerful, because the **sequence** we want to predict can be quite general! It can be composed of **words**, or of **characters**, or of **musical notes**, or of data points in a **time series** such as EKG voltages, or stock prices, or even a sequence of **DNA nucleotides**! \n",
        "\n",
        "We will train our model on the entire corpus of Shakespeare's Sonnets, and the model will learn from that data the most likely patterns of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "qk7oAJQasnXD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-06-15T18:18:20.453Z",
          "iopub.status.busy": "2020-06-15T18:18:20.442Z",
          "iopub.status.idle": "2020-06-15T18:18:20.513Z",
          "shell.execute_reply": "2020-06-15T18:18:20.523Z"
        },
        "id": "L-AX4IBIcqLK"
      },
      "source": [
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# import a custom text data preparation class\n",
        "!wget https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/main/module1-rnn-and-lstm/data_cleaning_toolkit_class.py\n",
        "from data_cleaning_toolkit_class import data_cleaning_toolkit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db6enoAacqLL"
      },
      "source": [
        "### Use [requests](https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request) to pull data from a URL\n",
        "\n",
        "Download the Shakespeare Sonnets from the [Gutenberg](https://www.gutenberg.org/cache/epub/1041/pg1041.txt) website. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6ac79c2e9a53d747ebf8fb41f4b39340",
          "grade": false,
          "grade_id": "cell-b8ececfad1f60557",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "nMf2XrJbcqLM"
      },
      "source": [
        "# download all of Shakespeare's Sonnets from the Project Gutenberg website\n",
        "url_shakespeare_sonnets = \"https://www.gutenberg.org/cache/epub/1041/pg1041.txt\"\n",
        "\n",
        "# use requests and the url to download all of the sonnets\n",
        "data = requests.get(url_shakespeare_sonnets)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4ab4f4f14188a9f3703d43d223bfa150",
          "grade": false,
          "grade_id": "cell-0cd0c8509bc8e8cf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dcXTQ5RTcqLM"
      },
      "source": [
        "# extract the downloaded text from the requests object and save to new variable\n",
        "raw_text_data = data.text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW4mj8eScqLN"
      },
      "source": [
        "# confirm the data type of `raw_text_data`\n",
        "assert(type(raw_text_data)==str)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwAX_OwEcqLN"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j7G1zqncqLO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "c6316239-9626-48d2-e1c6-c336d7e4c48d"
      },
      "source": [
        "# preview data to get an idea of how to begin cleaning\n",
        "raw_text_data[:1000]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of The Sonnets, by William Shakespeare\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org. If you are not located in the United States, you\\r\\nwill have to check the laws of the country where you are located before\\r\\nusing this eBook.\\r\\n\\r\\nTitle: The Sonnets\\r\\n\\r\\nAuthor: William Shakespeare\\r\\n\\r\\nRelease Date: September, 1997 [eBook #1041]\\r\\n[Most recently updated: November 25, 2021]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\nProduced by:  the Project Gutenberg Shakespeare Team\\r\\n\\r\\n*** START OF THE PROJECT GUTENBERG EBOOK THE SONNETS ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTHE SONNETS\\r\\n\\r\\nby William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  I\\r\\n\\r\\n  From fairest creatures we desire increase,\\r\\n  That thereby beautyâ€™s rose might never die,\\r\\n  But as the riper should by time decease'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "13b66e41cc64459f0757f6f53a78e08f",
          "grade": false,
          "grade_id": "cell-916f742d2cea299a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "JolH-nrVcqLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c3ac6d-fe6d-4039-bb59-4350fe9ce4d5"
      },
      "source": [
        "# split the text into separate lines and save the result to new variable\n",
        "split_data = raw_text_data.split('\\r\\n')\n",
        "split_data[:20]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\ufeffThe Project Gutenberg eBook of The Sonnets, by William Shakespeare',\n",
              " '',\n",
              " 'This eBook is for the use of anyone anywhere in the United States and',\n",
              " 'most other parts of the world at no cost and with almost no restrictions',\n",
              " 'whatsoever. You may copy it, give it away or re-use it under the terms',\n",
              " 'of the Project Gutenberg License included with this eBook or online at',\n",
              " 'www.gutenberg.org. If you are not located in the United States, you',\n",
              " 'will have to check the laws of the country where you are located before',\n",
              " 'using this eBook.',\n",
              " '',\n",
              " 'Title: The Sonnets',\n",
              " '',\n",
              " 'Author: William Shakespeare',\n",
              " '',\n",
              " 'Release Date: September, 1997 [eBook #1041]',\n",
              " '[Most recently updated: November 25, 2021]',\n",
              " '',\n",
              " 'Language: English',\n",
              " '',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop all the boiler plate text (i.e. titles and descriptions) and extra white spaces so we are left with only the sonnets themselves "
      ],
      "metadata": {
        "id": "pwRCjMVSnqii"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuGAL77acqLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671feeaa-db16-4f6a-e1d8-906ef60fb5f1"
      },
      "source": [
        "# find first line of sonnets\n",
        "split_data[:40]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\ufeffThe Project Gutenberg eBook of The Sonnets, by William Shakespeare',\n",
              " '',\n",
              " 'This eBook is for the use of anyone anywhere in the United States and',\n",
              " 'most other parts of the world at no cost and with almost no restrictions',\n",
              " 'whatsoever. You may copy it, give it away or re-use it under the terms',\n",
              " 'of the Project Gutenberg License included with this eBook or online at',\n",
              " 'www.gutenberg.org. If you are not located in the United States, you',\n",
              " 'will have to check the laws of the country where you are located before',\n",
              " 'using this eBook.',\n",
              " '',\n",
              " 'Title: The Sonnets',\n",
              " '',\n",
              " 'Author: William Shakespeare',\n",
              " '',\n",
              " 'Release Date: September, 1997 [eBook #1041]',\n",
              " '[Most recently updated: November 25, 2021]',\n",
              " '',\n",
              " 'Language: English',\n",
              " '',\n",
              " '',\n",
              " 'Produced by:  the Project Gutenberg Shakespeare Team',\n",
              " '',\n",
              " '*** START OF THE PROJECT GUTENBERG EBOOK THE SONNETS ***',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'THE SONNETS',\n",
              " '',\n",
              " 'by William Shakespeare',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '  I',\n",
              " '',\n",
              " '  From fairest creatures we desire increase,',\n",
              " '  That thereby beautyâ€™s rose might never die,',\n",
              " '  But as the riper should by time decease,',\n",
              " '  His tender heir might bear his memory:']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find last line of sonnets\n",
        "split_data[-360:-345]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akZ3n5JU0qmc",
        "outputId": "de6e87ca-b2ed-47d8-ba87-69740db09f43"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  And so the general of hot desire',\n",
              " '  Was, sleeping, by a virgin hand disarmâ€™d.',\n",
              " '  This brand she quenched in a cool well by,',\n",
              " '  Which from Loveâ€™s fire took heat perpetual,',\n",
              " '  Growing a bath and healthful remedy,',\n",
              " '  For men diseasâ€™d; but I, my mistressâ€™ thrall,',\n",
              " '    Came there for cure and this by that I prove,',\n",
              " '    Loveâ€™s fire heats water, water cools not love.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '*** END OF THE PROJECT GUTENBERG EBOOK THE SONNETS ***',\n",
              " '',\n",
              " 'Updated editions will replace the previous one--the old editions will']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fsYTC76cqLQ"
      },
      "source": [
        "**Use list index slicing to remove the titles and descriptions, so we only have the sonnets.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "00ead0a1024ff72116c24f6b473c1aac",
          "grade": false,
          "grade_id": "cell-1f388b88b0eec24a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "NLaFgX08cqLR"
      },
      "source": [
        "# find first and last lines of sonnet\n",
        "first_sonnet_line = '  From fairest creatures we desire increase,'\n",
        "last_sonnet_line = '    Loveâ€™s fire heats water, water cools not love.'\n",
        "\n",
        "# find index boundaries (start, end)\n",
        "start_index = split_data.index(first_sonnet_line)\n",
        "end_index = split_data.index(last_sonnet_line)\n",
        "\n",
        "# use index slicing to isolate the sonnet lines from the boiler plate text\n",
        "sonnets = split_data[start_index:end_index]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see how many lines were removed\n",
        "print(f\"Length of entire e-book: {len(split_data)} lines\")\n",
        "print(f\"Length of the Sonnets: {len(sonnets)} lines\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6wUivCIV5UM",
        "outputId": "1775f189-69f9-4201-d039-17a06c9eb3d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of entire e-book: 3004 lines\n",
            "Length of the Sonnets: 2615 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mJReim_43Ma"
      },
      "source": [
        "There are still many lines that should not be counted as part of the sonnets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th9BuvVlcqLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2caa3073-9197-497d-bda1-13ea86860540"
      },
      "source": [
        "# these non-sonnet lines have far fewer characters than the actual sonnet lines\n",
        "sonnets[200:220]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['    And nothing â€™gainst Timeâ€™s scythe can make defence',\n",
              " '    Save breed, to brave him when he takes thee hence.',\n",
              " '',\n",
              " '  XIII',\n",
              " '',\n",
              " '  O! that you were your self; but, love you are',\n",
              " '  No longer yours, than you your self here live:',\n",
              " '  Against this coming end you should prepare,',\n",
              " '  And your sweet semblance to some other give:',\n",
              " '  So should that beauty which you hold in lease',\n",
              " '  Find no determination; then you were',\n",
              " '  Yourself again, after yourselfâ€™s decease,',\n",
              " '  When your sweet issue your sweet form should bear.',\n",
              " '  Who lets so fair a house fall to decay,',\n",
              " '  Which husbandry in honour might uphold,',\n",
              " '  Against the stormy gusts of winterâ€™s day',\n",
              " '  And barren rage of deathâ€™s eternal cold?',\n",
              " '    O! none but unthrifts. Dear my love, you know,',\n",
              " '    You had a father: let your son say so.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "649cf52260448a5faf539ad6b6e8e6e8",
          "grade": false,
          "grade_id": "cell-84c4b3cf1f3c032a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "OiOfyPexcqLS"
      },
      "source": [
        "# use best judgement to decide on a good value for  \n",
        "# the minimum number of characters that a sonnet should have\n",
        "min_chars = 15\n",
        "\n",
        "# use 'min_chars' to filter out all the non-sonnet lines\n",
        "filtered_sonnets = [i for i in sonnets if len(i) > min_chars]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW_AFhCUcqLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed99e930-c519-4aff-c6ab-1f481cbba8aa"
      },
      "source": [
        "# view section of text to determine next cleaning steps\n",
        "filtered_sonnets[:20]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  From fairest creatures we desire increase,',\n",
              " '  That thereby beautyâ€™s rose might never die,',\n",
              " '  But as the riper should by time decease,',\n",
              " '  His tender heir might bear his memory:',\n",
              " '  But thou, contracted to thine own bright eyes,',\n",
              " '  Feedâ€™st thy lightâ€™s flame with self-substantial fuel,',\n",
              " '  Making a famine where abundance lies,',\n",
              " '  Thy self thy foe, to thy sweet self too cruel:',\n",
              " '  Thou that art now the worldâ€™s fresh ornament,',\n",
              " '  And only herald to the gaudy spring,',\n",
              " '  Within thine own bud buriest thy content,',\n",
              " '  And tender churl makâ€™st waste in niggarding:',\n",
              " '    Pity the world, or else this glutton be,',\n",
              " '    To eat the worldâ€™s due, by the grave and thee.',\n",
              " '  When forty winters shall besiege thy brow,',\n",
              " '  And dig deep trenches in thy beautyâ€™s field,',\n",
              " '  Thy youthâ€™s proud livery so gazed on now,',\n",
              " '  Will be a tatterâ€™d weed of small worth held:',\n",
              " '  Then being asked, where all thy beauty lies,',\n",
              " '  Where all the treasure of thy lusty days;']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iD4oqhVcqLT"
      },
      "source": [
        "### Use Custom Data Cleaning Tool \n",
        "\n",
        "We still need to remove all the punctuation and case normalize the text.\n",
        "\n",
        "Use the appropriate methods in the `data_cleaning_toolkit` to clean your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a722083a29139936744ff9a341e1c9a3",
          "grade": false,
          "grade_id": "cell-775c14b456d8a724",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "S7V1Q0h_cqLT"
      },
      "source": [
        "# instantiate the data_cleaning_toolkit class\n",
        "dctk = data_cleaning_toolkit()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ab91e612cd08068f3a36172979157d5d",
          "grade": false,
          "grade_id": "cell-684010b6a7360876",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "YquHh21GcqLT"
      },
      "source": [
        "# use data_cleaning_toolkit to remove punctuation and to case normalize\n",
        "clean_sonnets = [dctk.clean_data(text) for text in filtered_sonnets]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN3vWjofcqLT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "a723c651-9243-4af6-ddf9-a64f7404fcf5"
      },
      "source": [
        "# view cleaned sonnets\n",
        "display(clean_sonnets[:20])\n",
        "print(len(clean_sonnets))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['from fairest creatures we desire increase',\n",
              " 'that thereby beautys rose might never die',\n",
              " 'but as the riper should by time decease',\n",
              " 'his tender heir might bear his memory',\n",
              " 'but thou contracted to thine own bright eyes',\n",
              " 'feedst thy lights flame with selfsubstantial fuel',\n",
              " 'making a famine where abundance lies',\n",
              " 'thy self thy foe to thy sweet self too cruel',\n",
              " 'thou that art now the worlds fresh ornament',\n",
              " 'and only herald to the gaudy spring',\n",
              " 'within thine own bud buriest thy content',\n",
              " 'and tender churl makst waste in niggarding',\n",
              " 'pity the world or else this glutton be',\n",
              " 'to eat the worlds due by the grave and thee',\n",
              " 'when forty winters shall besiege thy brow',\n",
              " 'and dig deep trenches in thy beautys field',\n",
              " 'thy youths proud livery so gazed on now',\n",
              " 'will be a tatterd weed of small worth held',\n",
              " 'then being asked where all thy beauty lies',\n",
              " 'where all the treasure of thy lusty days']"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSfirtmYcqLT"
      },
      "source": [
        "### Use Your Data Tool to Create Character Sequences for the LSTM model\n",
        "\n",
        "The `create_char_sequences` method requires a parameter called `maxlen,` which is responsible for setting the maximum sequence length. \n",
        "\n",
        "To determine a good max sequence length, first calculate some statistics! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1deebea2ada0a7dc7d2eb08295ee1e2b",
          "grade": false,
          "grade_id": "cell-9ebdaa2654dd29ab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "O7Buzpw3cqLU"
      },
      "source": [
        "def calc_stats(corpus):\n",
        "    \"\"\"\n",
        "    Calculates statistics on the length of every line in the sonnets\n",
        "    \"\"\"\n",
        "    \n",
        "    # calculates each sonnet's line length\n",
        "    doc_lens = [len(line) for line in corpus]\n",
        "\n",
        "    # calculate and return the mean, median, std, max, min of the doc lengths\n",
        "\n",
        "    return [np.mean(doc_lens),\n",
        "            np.median(doc_lens),\n",
        "            np.std(doc_lens),\n",
        "            np.max(doc_lens),\n",
        "            np.min(doc_lens)]\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DGCJO_QcqLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6f937d-ce43-4014-99a9-5d1690e3046f"
      },
      "source": [
        "# sonnet line length statistics \n",
        "mean, med, std, max_, min_ = calc_stats(clean_sonnets)\n",
        "mean, med, std, max_, min_ "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40.87743732590529, 41.0, 4.041890872647064, 57, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "690957e46b6f2f32c1f17756d8ceab5b",
          "grade": false,
          "grade_id": "cell-35185e26897aad7e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "WOCylsbLcqLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcffa55-5f9e-48fa-9196-2bbfc1dc7a61"
      },
      "source": [
        "# from the results of the sonnet line length statistics\n",
        "# use judgement to select a value for maxlen\n",
        "\n",
        "# a good value could be half the median length of a sonnet line\n",
        "maxlen = 20\n",
        "dctk.create_char_sequences(clean_sonnets, maxlen=maxlen)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 18037 sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hutUpE9cqLV"
      },
      "source": [
        "Take a look at the `data_cleaning_toolkit_class.py` file. \n",
        "\n",
        "In the first 4 lines of code in the `create_char_sequences` method, class attributes `n_features` and `unique_chars` are created. <br>\n",
        "Call these two attributes in the cells below to see that the number of unique characters is the same as the number input features for our model because each of the unique characters is a possible prediction for this classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8BNLJuhcqLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d905fc66-bbfa-411d-ece9-0466ed43539a"
      },
      "source": [
        "# number of input features for our LSTM model\n",
        "dctk.n_features"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrPRQy3cqLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93463a0b-608d-4fcc-af4a-d0e1f309fddc"
      },
      "source": [
        "# unique characters that appear in our sonnets \n",
        "dctk.unique_chars"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['o',\n",
              " 'b',\n",
              " 'k',\n",
              " 'g',\n",
              " 'x',\n",
              " 'u',\n",
              " 'z',\n",
              " 'i',\n",
              " 's',\n",
              " 'w',\n",
              " 'e',\n",
              " 'h',\n",
              " 'm',\n",
              " 'd',\n",
              " ' ',\n",
              " 'y',\n",
              " 'r',\n",
              " 'l',\n",
              " 'j',\n",
              " 'f',\n",
              " 'v',\n",
              " 'a',\n",
              " 'q',\n",
              " 'c',\n",
              " 't',\n",
              " 'p',\n",
              " 'n']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXZMkLckcqLW"
      },
      "source": [
        "### Use Our Data Tool to Create X and Y Splits\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK-1GQzncqLW"
      },
      "source": [
        "# use data_cleaning_toolkit to separate X and y\n",
        "X, y = dctk.create_X_and_Y()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xtDCd8wcqLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c938907-e55d-4d59-fbe8-093ea05128bd"
      },
      "source": [
        "# our input array isn't a matrix - it's a rank three tensor\n",
        "X.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18037, 20, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0_YHHQdcqLX"
      },
      "source": [
        "In $X$.shape, we see three numbers (*n1*, *n2*, *n3*). \n",
        "\n",
        "*n1* tells us the number of samples that we have. But what about the other two?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB-n6_P4cqLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0b6bc0-2bd1-4a70-b69f-7ff06d0f9bba"
      },
      "source": [
        "# first index returns a single sample, which we can see is a sequence \n",
        "first_sample_index = 0 \n",
        "print(X[first_sample_index][:5])\n",
        "print(len(X[first_sample_index]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[False False False False False False False False False False False False\n",
            "  False False False False False False False  True False False False False\n",
            "  False False False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False  True False False False False False False False\n",
            "  False False False]\n",
            " [ True False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False]\n",
            " [False False False False False False False False False False False False\n",
            "   True False False False False False False False False False False False\n",
            "  False False False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False  True False False False False False False False False False\n",
            "  False False False]]\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A75Caji7cqLX"
      },
      "source": [
        "Notice that each sequence (i.e., $X[i]$ where $i$ is same index value) is `maxlen` long and <br>\n",
        "has a number of features equal to `dctk.n_features`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1E8AFAIcqLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8cf359-db92-4fcf-d54c-84f106115002"
      },
      "source": [
        "# each sequence is maxlen long and has dctk.n_features number of features\n",
        "X[first_sample_index].shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iChccLKWcqLX"
      },
      "source": [
        "**Each row corresponds to a character vector,** and there is `maxlen` number of character vectors. \n",
        "\n",
        "**Each column corresponds to a unique character,** and there are `dctk.n_features` number of features. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbWKVpd5cqLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709d6091-4c1c-4dfe-a941-130afc8983de"
      },
      "source": [
        "# index for a single character vector \n",
        "first_char_vect_index = 0\n",
        "X[first_sample_index][first_char_vect_index]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False,  True, False, False, False, False, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cVGE3wtcqLX"
      },
      "source": [
        "Notice that there is a single `True` value, and all the rest of the values are `False`. \n",
        "\n",
        "This is a one-hot encoding for which character appears at each index within a sequence. Specifically, the cell above is looking at the first character in the sequence.\n",
        "\n",
        "Only a single character can appear as the first character in a sequence, so there will be a single `True` value, and the rest will be `False`. \n",
        "\n",
        "Let's say that `True` appears in the $ith$ index; by  $ith$ index we mean some index in the general case. To find out which character each vector corresponds to, use the character-to-integer look-up dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ8dIATxcqLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0dc8ccf-2dbb-498e-caa2-e7abe45d22ff"
      },
      "source": [
        "# take a look at the index to character dictionary\n",
        "# if a TRUE appears in the 0th index of a character vector,\n",
        "# then we know that whatever char you see below next to the 0th key \n",
        "# is the character that character vector is encoding for\n",
        "dctk.int_char"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'o',\n",
              " 1: 'b',\n",
              " 2: 'k',\n",
              " 3: 'g',\n",
              " 4: 'x',\n",
              " 5: 'u',\n",
              " 6: 'z',\n",
              " 7: 'i',\n",
              " 8: 's',\n",
              " 9: 'w',\n",
              " 10: 'e',\n",
              " 11: 'h',\n",
              " 12: 'm',\n",
              " 13: 'd',\n",
              " 14: ' ',\n",
              " 15: 'y',\n",
              " 16: 'r',\n",
              " 17: 'l',\n",
              " 18: 'j',\n",
              " 19: 'f',\n",
              " 20: 'v',\n",
              " 21: 'a',\n",
              " 22: 'q',\n",
              " 23: 'c',\n",
              " 24: 't',\n",
              " 25: 'p',\n",
              " 26: 'n'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXy52FuHcqLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf23abe-eafa-44e8-a544-d2e5ef745852"
      },
      "source": [
        "# let's look at an example to tie it all together\n",
        "seq_len_counter = 0\n",
        "\n",
        "# index for a single sample \n",
        "for seq_of_char_vects in X[first_sample_index]:\n",
        "    \n",
        "    # get index with max value, which will be the one TRUE value \n",
        "    index_with_TRUE_val = np.argmax(seq_of_char_vects)\n",
        "    \n",
        "    print (dctk.int_char[index_with_TRUE_val])\n",
        "    \n",
        "    seq_len_counter+=1\n",
        "    \n",
        "print (\"Sequence length: {}\".format(seq_len_counter))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f\n",
            "r\n",
            "o\n",
            "m\n",
            " \n",
            "f\n",
            "a\n",
            "i\n",
            "r\n",
            "e\n",
            "s\n",
            "t\n",
            " \n",
            "c\n",
            "r\n",
            "e\n",
            "a\n",
            "t\n",
            "u\n",
            "r\n",
            "Sequence length: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-H9jSq6cqLY"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Fo7tn5cqLY"
      },
      "source": [
        "### Build a Shakespeare Sonnet Text Generation Model\n",
        "\n",
        "Now that we have prepped our data, let's finally build out our character generation model.<br>\n",
        "\n",
        "First, we'll create a callback to monitor the training -- by printing a sample of text generated by the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to generate a sample character:"
      ],
      "metadata": {
        "id": "kVc4tb1OSMMr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xgk-JomzwGX"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Helper function to generate a sample character\n",
        "    Input is a predictions vector from our model,\n",
        "    for example a set of 27 character probabilities\n",
        "    Output is the index of the generated character \n",
        "    \"\"\"\n",
        "    # convert predictions to an array \n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "\n",
        "    # use the temperature hyper-parameter to \"warp\" \n",
        "    # (sharpen or spread out) the probability distribution \n",
        "    preds = np.log(preds) / temperature\n",
        "\n",
        "    # use the softmax activation function to create a new list of probabilities \n",
        "    # corresponding to the \"warped\" probability distribution\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "    # Draw a single sample from a multinomial distribution, given these probabilities\n",
        "    #   The sample will be a one-hot encoded character\n",
        "    \"\"\" Notes on the np.random.multinomial() function \n",
        "       The first argument is the number of \"trials\" we want: 1 in this case\n",
        "       The second argument is the list of probabilities for each character\n",
        "       The third argument is number of sets of \"trials\" we want: 1 in this case\n",
        "       By analogy with a dice-rolling experiment: \n",
        "\n",
        "       This \"trial\" consists of generating a single \"throw\" of a 27-sided die;\n",
        "       each face corresponds to a character and its associated probability\n",
        "    \"\"\"\n",
        "\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    \n",
        "    # return the index that corresponds to the max probability \n",
        "    return np.argmax(probas)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPhu860ZEd3g"
      },
      "source": [
        "Create the `on_epoch_end` function to be passed into `LambdaCallback()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9cBsweJcqLY"
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    \"\"\"\"\n",
        "    Function invoked at the end of each epoch.\n",
        "    Prints the text generated by our model.\n",
        "    \"\"\"\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "\n",
        "    # randomly pick a starting index \n",
        "    # will be used to take a random sequence of chars from `text`\n",
        "    start_index = random.randint(0, len(text) - dctk.maxlen - 1)\n",
        "    \n",
        "    # this is our seed string (i.e. input sequence into the model)\n",
        "    generated = ''\n",
        "\n",
        "    # start the sentence at index `start_index` and \n",
        "    # include the next `dctk.maxlen` number of chars\n",
        "    sentence = text[start_index: start_index + dctk.maxlen]\n",
        "\n",
        "    # add to generated\n",
        "    generated += sentence\n",
        "\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    # use model to predict what the next maxlen \n",
        "    # chars should be that follow the seed string\n",
        "    for i in range(maxlen):\n",
        "\n",
        "        # shape of a single sample in a rank 3 tensor \n",
        "        x_dims = (1, dctk.maxlen, dctk.n_features)\n",
        "        # create an array of zeros with shape x_dims\n",
        "        # recall that python considers zeros and boolean FALSE as the same\n",
        "        x_pred = np.zeros(x_dims)\n",
        "\n",
        "        # create a seq vector for our randomly select sequence \n",
        "        # i.e. create a numerical encoding for each char in the sequence \n",
        "        for t, char in enumerate(sentence):\n",
        "            # for sample 0 in seq index t and character `char`\n",
        "            # encode a 1 (which is the same as a TRUE)\n",
        "            x_pred[0, t, dctk.char_int[char]] = 1\n",
        "\n",
        "        # pass the sequence vector into the model to get\n",
        "        # a prediction of what the next char should be \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        \n",
        "        # use the sample helper function to get index for next char \n",
        "        next_index = sample(preds)\n",
        "        # use look up dict to get next char \n",
        "        next_char = dctk.int_char[next_index]\n",
        "\n",
        "        # append next char to sequence \n",
        "        sentence = sentence[1:] + next_char \n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6xJijWAcqLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cf7fa7-8dfe-4a39-f5bf-137a58fa39f9"
      },
      "source": [
        "# need this for on_epoch_end()\n",
        "text = \" \".join(clean_sonnets)\n",
        "print(f'All of Shakespeare\\'s sonnets comprise about {len(text)} characters.')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All of Shakespeare's sonnets comprise about 90203 characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the callback object"
      ],
      "metadata": {
        "id": "91FywCwoUzn0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKDdNLhBcqLZ"
      },
      "source": [
        "# create callback obj that will print text generation at the end of each epoch \n",
        "# use for real-time monitoring of model performance\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cq75YmTcqLZ"
      },
      "source": [
        "----\n",
        "### Build and Train Model\n",
        "\n",
        "Build a text generation model using LSTMs.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build text generation model layer by layer \n",
        "model = Sequential([\n",
        "                    LSTM(128,\n",
        "                         input_shape=(dctk.maxlen, dctk.n_features),\n",
        "                         activation='relu',\n",
        "                         return_sequences=True),\n",
        "                    LSTM(64,\n",
        "                         activation='relu',\n",
        "                         return_sequences=False),\n",
        "                    Dense(dctk.n_features,\n",
        "                          activation='softmax')\n",
        "                    ])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam')\n",
        "\n",
        "# fit model\n",
        "history = model.fit(X, y,\n",
        "          batch_size=200,\n",
        "          epochs=100,\n",
        "          callbacks=[print_callback])"
      ],
      "metadata": {
        "id": "zNso-N_5e1KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94I0bGvfJQ0a"
      },
      "source": [
        "### Save the trained model to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4amWje1ncqLZ"
      },
      "source": [
        "# save trained model to file \n",
        "model.save(\"trained_text_gen_model.h5\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWWRV1VJcqLa"
      },
      "source": [
        "### Try Out the Trained Model \n",
        "\n",
        "Now that we have a trained model that, though far from perfect, can generate actual English words, we can look at the predictions to continue learning more about how a text generation model works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gktQ5JQqcqLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "98dbdbdb-54ed-4b78-b9a2-c9dbadeead7c"
      },
      "source": [
        "# this is our joined clean sonnet data\n",
        "text[:300]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a famine where abundance lies thy se'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSGHFS4QcqLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f917376-0a46-4aa5-cfae-af1d464f5f65"
      },
      "source": [
        "# randomly pick a starting index \n",
        "# will be used to take a random sequence of chars from `text`\n",
        "start_index = random.randint(0, len(text) - dctk.maxlen - 1)\n",
        "start_index"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21597"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ak7epOKcqLa"
      },
      "source": [
        "# use the randomly selected starting index to sample a sequence from the `text`\n",
        "\n",
        "# this is our seed string (i.e., input sequence into the model)\n",
        "generated = ''\n",
        "\n",
        "# start the sentence at index `start_index` and\n",
        "# include the next `dctk.maxlen` number of chars\n",
        "sentence = text[start_index: start_index + dctk.maxlen]\n",
        "\n",
        "# add to generated\n",
        "generated += sentence"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfbUcUBXcqLa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fc0de3-ee19-4568-c107-7bb26ddcf616"
      },
      "source": [
        "# display the \"seed string\" i.e. the input sequence into the model\n",
        "print('----- Input seed: \"' + sentence + '\"')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Input seed: \"itled in thy parts d\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nop-G7CcqLa"
      },
      "source": [
        "# use model to predict what the next maxlen \n",
        "# chars should be that follow the seed string\n",
        "for i in range(maxlen):\n",
        "\n",
        "    # shape of a single sample in a rank 3 tensor \n",
        "    x_dims = (1, dctk.maxlen, dctk.n_features)\n",
        "    # create an array of zeros with shape x_dims\n",
        "    x_pred = np.zeros(x_dims)\n",
        "\n",
        "    # create a seq vector for our randomly selected sequence \n",
        "    for t, char in enumerate(sentence):\n",
        "        # for sample 0 in seq index t and character `char`\n",
        "        # encode a 1 (which is the same as a TRUE)\n",
        "        x_pred[0, t, dctk.char_int[char]] = 1\n",
        "\n",
        "    # take the seq vector and pass into model to get\n",
        "    # a prediction of what the next char should be \n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    \n",
        "    # use the sample helper function to get index for next char \n",
        "    next_index = sample(preds)\n",
        "    # use look up dict to get next char \n",
        "    next_char = dctk.int_char[next_index]\n",
        "\n",
        "    # append next char to sequence \n",
        "    sentence = sentence[1:] + next_char "
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX9daz2rcqLb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "118576b6-d1b9-40b7-e995-8b7bd3f07268"
      },
      "source": [
        "# this is the seed string\n",
        "generated"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'itled in thy parts d'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3jnYVwccqLb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3b1488d0-f2da-4279-a5b2-3a49663c997b"
      },
      "source": [
        "# these are the maxlen chars the model thinks should come after the seed string\n",
        "sentence"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oth which treem fair'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1aqFH4BcqLb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "05b76b6e-edd2-4032-e368-d6a50b318e7e"
      },
      "source": [
        "# how put it all together\n",
        "generated + sentence"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'itled in thy parts doth which treem fair'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch Goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g., plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n"
      ]
    }
  ]
}